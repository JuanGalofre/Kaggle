Kaggle notes:
  I can do 
    print(dir(<object or function>)) also help(<object or function>)
  on an unkwnown object and it will appear what can i do with said object
Random forest model
  Model where we create several trees that indivually consider each passanger data and vote on whether the individual survived.

Decision tree model
  Step of capturing patterns from data is called fitting or training the model. 
  The data used to fit the model is called training data. And the data that we use is the training data
  If the tree has more splits, so if it takes into account more data, it can be said that it is a deeper tree
  The point at the bottom where we make the decision is called a leaf
  The depth of the tree is really important, as it determines how many variables it takes into account to make a prediction. For each split that we have
  in our tree, the amount of leafs doubles. So to calculate how mean leafs we will have, it's just to do 2**n, where n is the number of splits or variables
  we are going to take into account.
  Here lies as problem, as we get more divisions, that means, that our data is getting divided as well. That means if we use a lot of variables, or if we     have a lot of depth in our tree, we will have little to none data in each resulting leaf. That means that each leaf is going to mimic the training data, but
  it's going to perform poorly when it comes to external data. That is overfitting
  Also underfitting happens when we have to little leafs, meaning that the data is yet to be classified. That means that we will have a poor performance in
  both sets.
  So, what they do is to control the amount of leafs. What is kinda weird is how we can control the amount of leaves and at the same time, have the same 
  variables in it.
    Model was built like: 
      model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    Then we can explore between leaf amounts like:
      for max_leaf_nodes in [5, 50, 500, 5000]:
        my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
        print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))
  So what we can do is to find the best amount of leaf nodes by the common way, that means, splitting our training data into train and test. And then we 
  use the whole data to create our model for evaluation
Random Forest
  It uses many trees, and make the prediction based on the average of those trees.
  Random forest regressor

4 steps for building a model
  Define: What type of model will it be ? 
  Fit: Capture patterns from the data
  Predict
  Evaluate: Determine how accurate the model predictions are
  When we want to use our model in practice, we can use all our data to train it, instead of splitting the data into training and test datasets
Model Validation
  Mean absolute error aka MAE
  error is error=actual-predicted
  So if a house cost 150 and my model predicted 100, then the error will be 50 
  MAE- Take the absolute value of each error. And then we take the average of those absolute errors.
  "On average, our predictions are off by about X"
  There is an built in function to do it. 
  from sklearn.metrics import mean_absolute_error
  It is no so good to use the training set as the test set. As we need to have different training data vs testing data. Because some patterns can only be true in the training data.

Definitions
  Features: Columns that are inputted into our model and later used to make predictions
  Target: Column which has the data that we are going to predict
  Validation data -> Data that is excluded from the model-building process, in order to test the model with it
  Overfitting -> When the model has so little data in each leaf, that we match the training set perfectly. But that also means that we have problems with external data


Usefull functions
  DataFrame.describe()-> describes the dataset
  DataFrame.columns().to_list() -> get a list with all the headers of the data frame
  DataFrame.column -> Obtiene la columna como una variable, exactamente como un Series
  I can select which columns do i want from a dataframe by doing DataFrame[columns]. That way i get a data frame only whit the values i'm interested in
  Some models have a random_state value, that is like a seed, so we can make the model deterministic. It is considered as a good practice 
  mean_absolute_error(y,predicted_y) will compare and throw the difference between our y and the predicted_y in terms of numbers.
  train_test_split will split the data of our training set into training and test set.
    One liner to do that train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)


Pandas
  DataFrame and series are the two core objects of pandas
  A pandas dataframe can be done like:
    pd.DataFrame({'Yes':[1,2,3,4,5],'No':{2,3,4,5,6}})
  Where the Key is the label of the column, and the values are the list that is beside it.
  Index by default is numeric, from 0 to .., but we can specify it using
    pd.DataFrame({'Yes':[1,2,3,4,5],
                  'No':{2,3,4,5,6}},
                  index=['Stuff A','Stuff B'])
  But Index and those columns should have the same size.
  Series -> pd.Series([1,2,3,4,5]). It is a single column of a datafrmae. So the stuff that we did in the pd.DataFrame are actually series
  So series can have an index, and a name
  pd.Series([30, 35, 40], 
            index=['2015 Sales', '2016 Sales', '2017 Sales'], 
            name='Product A')
  df.shape() -> How many columns and rows the df has
  df.head() -> First 5 rows of the Df
  A series also can be done of a join of 2 lists, like 
    quantities = ['4 cups', '1 cup', '2 large', '1 can']
    items = ['Flour', 'Milk', 'Eggs', 'Spam']
    recipe = pd.Series(quantities, index=items, name='Dinner')
  While we read a csv, we can asume that the first column is the index. By loading it
  by pd.read_csv("", index_col=0)
  Columns in pandas are attributes of the data frame, so we can access a column of said dataframe by a dot. df.column. Also it can be done using the dictionary notation, like df['column'].
  Also, as we access the df['column'] what we are accesing is a series. So inside a series
we can select the value that we want by treating it like a list.
  So df['column'][0] will access de dataframe 'column' series, and access the first value that is in it
  INDEXING, SELECTING AND ASSIGNING
    INDEX BASED SELECTION
      Pandas objects can be accesed through indexes. Like a list. It can be done when working with df.iloc[0]. Both loc and iloc are row first, column second. But that's the opposite of what we commonly do in python, whre it is column first, row second.
      We can use the : operator, that means everything.
        So we can retireve all the rows in a said column by using iloc in this way
          df.iloc[:,0]. So i will get all the rows of the 0 column
      We can add positions to said operator, to explicit the limits of that everything. That means that if we need just the first 3 elements, we can obtain them through
        df.iloc[:3,0]-> we will ge the the first 3 elements in the row, for the column 0
      We need to remember that python indexes start at 0, so if we want to get just the second and the third entry, we will use the : operator like:
        df.iloc[1:3,0]. Here the 1 represents the second element. And the 3 represents the fourth element. But in this operator, the left number is inclusive, and the right number is not inlusive, so we will stop at n-1. Tha means we start at 1 and we stop at 2
      A list can also be passed along
        df.iloc[[0,1,2],0]
      Negative numbers can be used to, and it will begin at the end of the file
        df.iloc[-5:]
    LABEL BASED SELECTION
      Here we are working with the data index value. So the title
      To get the first value of the 'country' column
        df.loc[0,'country'] -> Rows first, columns second
      And we can make use of that style of selection when we want to get info of some columns that we now.
        reviews.loc[:, ['taster_name', 'taster_twitter_handle', 'points']]-> Here, as we now the names of those columns, we can get all rows(:) for those columns
      But here, when we use the : operator, it works differently. It now becomes inclusive. So if i do 1:3, i will also get the 4th element.
        That operator works in letters so. So let's say we wanna select food in alfabetical order. If we want to select from A to P, and specifically from Apples to Potatoes, we can use
          df.loc['Apples':'Potatoes']
      So, here, index becomes important and there are methods to deal with it, like setting an index from a column that we have 
        reviews.set_index('title')
    CONDITIONAL SELECTION (filtering)
      As we select the column of a dataframe using attributes. We can use this to compare to a constant
        review.country=='Italy'
      That comparison will yield a series of True/False booleans based on the country of each record.
        It is a series like [True,False,True,True,False] and so on
      When we want to obitain objects in the data frame, we need to use .loc to get those columns
        df.loc[df.country=='Italy']
      I guess this is because we are selecting the rows where country == 'Italy'. And I guess is selecting the rows because we put a first argument in there, which are commonly rows.
      We can also use logical operators in it
        df.loc[(df.country =='Italy') & (df.points >= 90)] it is quite interesting that we have to put the arguments of the dataframe inside of the dataframe. I mean that we have to use the dataframe to filter stuff in it. THe first condition. That will give us a Series. The second one also wil give us a Series object. And then we have to use those Series objects and putting them in the dataframe again. Is not like a filter, is like we are giving the indiciation of which columns do we wanna show 
        df.loc[(df.country=='Italy')|(df.points>=90)]
      Pandas has its own conditional selectors.
        isin let us know if some data is inside a list of values.
          df.loc[df.country.isin(['Italy','France'])]
        isnull and notnull: Self explanatory but really important
          df.loc[df.df.column.notull()]
  ASSIGNING DATA 
    To put an assignment to the whole column, we go to the pandas series object and 
      df['column']='value'. And then all the Series will have the value 'value'
    We can also assign a list of values 
      df['column']=range(len(reviews),0)
    Here it appears something really really important, and that is the length of a series. If we are going to assign values to a series, it must be of the same length of said series. It is really important, as otherwise we will get an error
  MISTAKES
    This right here can be kind of tricky. We just need to remember that it is better to put the brackets on each expression
      top_oceania_wines = reviews[((reviews.country=="Australia")|(reviews.country=="New Zealand"))& (reviews.points>=95)]
    Another tricky one. About accesing with loc
      This is valid 
        df = reviews.loc[:99,['country','variety']]
      This is also valid 
        indexes=range(100) df = reviews.loc[index,['country','variety']]
      But this is not valid        
        df = reviews.loc[[:99],['country','variety']]


  


