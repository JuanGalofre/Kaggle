Kaggle notes:
  I can do 
    print(dir(<object or function>)) also help(<object or function>)
  on an unkwnown object and it will appear what can i do with said object
Random forest model
  Model where we create several trees that indivually consider each passanger data and vote on whether the individual survived.

Decision tree model
  Step of capturing patterns from data is called fitting or training the model. 
  The data used to fit the model is called training data. And the data that we use is the training data
  If the tree has more splits, so if it takes into account more data, it can be said that it is a deeper tree
  The point at the bottom where we make the decision is called a leaf
  The depth of the tree is really important, as it determines how many variables it takes into account to make a prediction. For each split that we have
  in our tree, the amount of leafs doubles. So to calculate how mean leafs we will have, it's just to do 2**n, where n is the number of splits or variables
  we are going to take into account.
  Here lies as problem, as we get more divisions, that means, that our data is getting divided as well. That means if we use a lot of variables, or if we     have a lot of depth in our tree, we will have little to none data in each resulting leaf. That means that each leaf is going to mimic the training data, but
  it's going to perform poorly when it comes to external data. That is overfitting
  Also underfitting happens when we have to little leafs, meaning that the data is yet to be classified. That means that we will have a poor performance in
  both sets.
  So, what they do is to control the amount of leafs. What is kinda weird is how we can control the amount of leaves and at the same time, have the same 
  variables in it.
    Model was built like: 
      model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    Then we can explore between leaf amounts like:
      for max_leaf_nodes in [5, 50, 500, 5000]:
        my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
        print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))
  So what we can do is to find the best amount of leaf nodes by the common way, that means, splitting our training data into train and test. And then we 
  use the whole data to create our model for evaluation
Random Forest
  It uses many trees, and make the prediction based on the average of those trees.
  Random forest regressor

4 steps for building a model
  Define: What type of model will it be ? 
  Fit: Capture patterns from the data
  Predict
  Evaluate: Determine how accurate the model predictions are
  When we want to use our model in practice, we can use all our data to train it, instead of splitting the data into training and test datasets
Model Validation
  Mean absolute error aka MAE
  error is error=actual-predicted
  So if a house cost 150 and my model predicted 100, then the error will be 50 
  MAE- Take the absolute value of each error. And then we take the average of those absolute errors.
  "On average, our predictions are off by about X"
  There is an built in function to do it. 
  from sklearn.metrics import mean_absolute_error
  It is no so good to use the training set as the test set. As we need to have different training data vs testing data. Because some patterns can only be true in the training data.

Definitions
  Features: Columns that are inputted into our model and later used to make predictions
  Target: Column which has the data that we are going to predict
  Validation data -> Data that is excluded from the model-building process, in order to test the model with it
  Overfitting -> When the model has so little data in each leaf, that we match the training set perfectly. But that also means that we have problems with external data


Usefull functions
  DataFrame.describe()-> describes the dataset
  DataFrame.columns().to_list() -> get a list with all the headers of the data frame
  DataFrame.column -> Obtiene la columna como una variable, exactamente como un Series
  I can select which columns do i want from a dataframe by doing DataFrame[columns]. That way i get a data frame only whit the values i'm interested in
  Some models have a random_state value, that is like a seed, so we can make the model deterministic. It is considered as a good practice 
  mean_absolute_error(y,predicted_y) will compare and throw the difference between our y and the predicted_y in terms of numbers.
  train_test_split will split the data of our training set into training and test set.
    One liner to do that train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)


  


